---
layout: post
title: 聊聊Raft一致性算法2
date: 2017-12-12
author: wuxiangwei
categories: 算法 Raft
tags: 算法 Raft
---

* Kramdown table of content
{:toc, toc}


**术语**

| 中文   | 英文  | 含义                                                       |
| :--    | :--   | :--                                                        |
| 日志   | Log   | 由日志项构成                                               |
| 日志项 | Entry | 包含index、term、command的一组数据，对应于client的一个请求 |


### Log replication ###

#### Log Matching Property ####

> Log Matching Property
> 1. If two entries in different logs have the same index and term, then they
>    store the same command.
> 2. If two entries in different logs have the same index and term, then the
>    logs are identical in all preceding entries.

在任意两个节点，对具有相同index和term的两条日志项，满足下面两个条件：

1. 这两条日志项的内容(command)相同；
2. 这两个节点中，这条日志项前面的所有日志项也一一对应并且相同。

对第一条，首先日志项由Leader创建，对给定的index和term，Leader最多只会创建1条日志项。并且，Leader不会修改日志项的index。
对第二条，Leader发送AppendEntries RPC时会携带前一条日志项的index和term（即prevLogIndex和prevLogTerm），Follower接收RPC后会检查本地是否包含prevLogIndex和prevLogTerm指定的日志项，如果不存在则拒绝RPC。这个过程称为一致性检查，通过这个检查可以推导出第二条。首先，初始情况每个节点都没有日志项，满足Log Matching条件；后面，根据一致性检查来保证条件2。


这个特性保证日志项间不存在空洞。

正常情况，Leader的Logs和所有Follower的Logs保持一致。
不过，在Leader反复故障，多次Leader选举后，各节点的Logs会出现不一致，参考论文的Figure7。

节点f在term为2时当选为Leader，从client接收3条日志项，持久化到本地，但都没复制到其它节点就挂掉了。挂掉后重新开始Leader选举，并且再次当选为Leader，此时term为3，从client接收5条日志项后再次挂掉，挂点后一直没起来，并且term为3时接收的日志项都没有复制到其它节点。

接下来，节点e当选为新Leader，term为4，处理了4条日志项后挂掉，再也没起来。这4条日志项中，前两条复制到多数派，后两条只记录到本地。
接下来，节点d当选为新Leader，term为5，处理了2条日志项后挂掉。不过这两条日志被复制到了多数派。
接下来，节点c当选为新Leader，term为6，处理了4条日志项后挂掉。这4条日志项中，前两条复制到了多数派，第3条只复制到了节点d，最后1条在本地。
接下来，节点d当选为新Leader，term为7，处理了2条日志项后挂掉。不过这两条日志项只在本地，没有复制到其它节点。
接下来，产生term8，就称为了Figure7中的状态。

| Term | Leader | 本地 | 多数派 |
| :--  | :--    | :--  | :--    |
| 2    | f      | 3    | 0      |
| 3    | f      | 5    | 0      |
| 4    | e      | 4    | 2      |
| 5    | d      | 2    | 2      |
| 6    | c      | 4    | 2      |
| 7    | d      | 2    | 0      |

**注**：本地列是指复制到Leader本地的日志项数量，多数派是指复制到多数派的日志项数量。

**问题**：新Leader当选后不会去同步数据，而是直接开始服务？


#### 不一致Log的处理 ####

> In Raft, the leader handles inconsistencies by forcing the followers'logs to
> duplicate its own.

Leader强制让Followers的Logs和自己保持一致，对Follower中不一致的日志项，删除。大致做法分两步：首先，从后往前找到第1条一致的日志项，根据Log Matching Property，这条日志项前面的所有日志项都跟Leader保持一致了。对这条日志项后面的日志项，先在Follower中删除，然后从Leader复制。

从具体实现来说，Leader为每个Follower维护1个*nextIndex*索引，Leader当选后先将*nextIndex*初始化为本地最后1条日志项加1。如果Follower的日志和Leader日志不一致，那么它会拒绝AppendEntries RPC，Leader被拒绝后递减对应的*nextIndex*索引，再继续尝试。最后，到达两者一致的那条日志项时，AppendEntry RPC会成功，此时在Follower中删除不一致的日志项，使Leader和Follower的日志一致。


**Tips**

对不一致的日志，前面所述方法，1个AppendEntries RPC只能检查出1条不一致的日志项，效率低。Follower发现不一致时将不一致日志项所在的term一并返回，Leader直接跳过这个term中的所有日志项。这种优化，在复制Leader时可能会重复发送一致的日志项。这时要求RPC满足幂等性，Follower要丢弃已经存在的日志项。


ceph paxos的处理方式

### Safety ###

**目标**：保证每个节点状态机执行命令的顺序相同。

#### Leader Completeness Property ####

> If a log entry is committed in a given term, then that entry will be present
> in the logs of the leaders for all higher-numbered terms.

新Leader的Log中包含所有已commit的日志项。

两种方法可以达到这个目的：一种是新Leader选举出来后开始服务前先向所有Follower同步数据，第二种是Leader选举过程中直接选择一个日志中包含所有已commit日志项的节点。Raft选择第二种方法，Ceph Paxos选择第一种方法。

1条日志项被commit意味着这条日志项已经被复制到了多数派。
1个节点当选为Leader也必须有多数派的支持。也就是说，这个多数派中至少有1个节点包含了最近commit的日志项，而节点投票给Leader的条件是Leader最近commit的日志项要不比本节点的最近commit的日志项旧。所以Leader包含了最近commit的日志项，从而包含所有已commit的日志项。

具体实现来说，RequestVote RPC包含lastLogIndex和lastLongTerm，如果Follower接收到后与自己进行比较，如果对方比较旧，则拒绝投票给它。


参考论文Figure8：

| 阶段 | term | Leader               | 日志项                      |
| :--  | :--  | :--                  | :--                         |
| a    | 2    | S1                   | 2被复制到S1、S2             |
| b    | 3    | S5                   | 3写到S5本地                 |
| c    | 4    | S1                   | 2被复制到S1、S2、S3         |
| d    | 5    | S5（S2、S3、S4投票） | 3被复制到S1、S2、S3、S4、S5 |

问题：在阶段c中，日志项2已经commit，但在阶段d中，日志项2被覆盖删除。















