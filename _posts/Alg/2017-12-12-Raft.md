---
layout: post
title: 聊聊Raft一致性算法2
date: 2017-12-12
author: wuxiangwei
categories: 算法 Raft
tags: 算法 Raft
---

* Kramdown table of content
{:toc, toc}


**术语**

| 中文   | 英文  | 含义                                                       |
| :--    | :--   | :--                                                        |
| 日志   | Log   | 由日志项构成                                               |
| 日志项 | Entry | 包含index、term、command的一组数据，对应于client的一个请求 |

### Leader Election ###

Leader选举

1. 触发选举的机制（心跳超时，Follower发起一轮选举）；
2. Server启动时为Follower角色；


发起选举的准备工作：
1. 增加currentTerm；
2. 角色从Follower转换为Candidate；
3. 投票给自己；
4. 并发发送RequestVote RPC给其它Server；

RequestVote RPC:
Arguments：
term  候选人的term
candidateId
lastLogIndex  候选人最近一条log的索引
lastLogTerm   候选人最近一条log的term

Results:
term  currentTerm 用于让候选人更新自己
voteGranted 是否投票给候选人


目标：
1. 给定的term最多只产生1个Leader （多数派规则）
2. Leader拥有最新的Log，选举结束就能提供服务，不需要同步数据。

当选为Leader的条件：收到多数派的投票。
多数派能够保证，对给定的term最多只会产生1个Leader。


投票规则：
1. 1个Term只能投1票；（投多票就会产生多个多数派）
2. 先到先得；
3. 候选人的term不小于currentTerm；
4. 候选人的log至少和自己相同新，或者更新；


CASE1：
候选人赢得选举。
赢得选举后，向其它所有Server发送心跳，避免其它节点超时触发新选举。


CASE2：
选举期间收到其它Server成为Leader的RPC：
1. 如果新Leader的term不小于候选人的currentTerm，承认新Leader的合法性，切换为Follower角色；
2. 如果新Leader的term小于候选人的currentTerm，拒绝RPC，继续呆在Candidate角色。

CASE3：
候选人既没有赢得选举也没有输掉选举。
举个例子：多个Followers同时发起选举，把票都投给了自己，从而无法形成一个多数派。
出现这种情况时，每个候选人都会超时，超时后再次增加term发起新一轮的选举。
每个Server的选举超时时间是给定范围内的一个随机值，这可以降低split vote问题出现的概率。

### Log replication ###

#### Log Matching Property ####

> Log Matching Property
> 1. If two entries in different logs have the same index and term, then they
>    store the same command.
> 2. If two entries in different logs have the same index and term, then the
>    logs are identical in all preceding entries.

在任意两个节点，对具有相同index和term的两条日志项，满足下面两个条件：

1. 这两条日志项的内容(command)相同；
2. 这两个节点中，这条日志项前面的所有日志项也一一对应并且相同。

对第一条，首先日志项由Leader创建，对给定的index和term，Leader最多只会创建1条日志项。并且，Leader不会修改日志项的index。
对第二条，Leader发送AppendEntries RPC时会携带前一条日志项的index和term（即prevLogIndex和prevLogTerm），Follower接收RPC后会检查本地是否包含prevLogIndex和prevLogTerm指定的日志项，如果不存在则拒绝RPC。这个过程称为一致性检查，通过这个检查可以推导出第二条。首先，初始情况每个节点都没有日志项，满足Log Matching条件；后面，根据一致性检查来保证条件2。


这个特性保证日志项间不存在空洞。

正常情况，Leader的Logs和所有Follower的Logs保持一致。
不过，在Leader反复故障，多次Leader选举后，各节点的Logs会出现不一致，参考论文的Figure7。

节点f在term为2时当选为Leader，从client接收3条日志项，持久化到本地，但都没复制到其它节点就挂掉了。挂掉后重新开始Leader选举，并且再次当选为Leader，此时term为3，从client接收5条日志项后再次挂掉，挂点后一直没起来，并且term为3时接收的日志项都没有复制到其它节点。

接下来，节点e当选为新Leader，term为4，处理了4条日志项后挂掉，再也没起来。这4条日志项中，前两条复制到多数派，后两条只记录到本地。
接下来，节点d当选为新Leader，term为5，处理了2条日志项后挂掉。不过这两条日志被复制到了多数派。
接下来，节点c当选为新Leader，term为6，处理了4条日志项后挂掉。这4条日志项中，前两条复制到了多数派，第3条只复制到了节点d，最后1条在本地。
接下来，节点d当选为新Leader，term为7，处理了2条日志项后挂掉。不过这两条日志项只在本地，没有复制到其它节点。
接下来，产生term8，就称为了Figure7中的状态。

| Term | Leader | 本地 | 多数派 |
| :--  | :--    | :--  | :--    |
| 2    | f      | 3    | 0      |
| 3    | f      | 5    | 0      |
| 4    | e      | 4    | 2      |
| 5    | d      | 2    | 2      |
| 6    | c      | 4    | 2      |
| 7    | d      | 2    | 0      |

**注**：本地列是指复制到Leader本地的日志项数量，多数派是指复制到多数派的日志项数量。

**问题**：新Leader当选后不会去同步数据，而是直接开始服务？


#### 不一致Log的处理 ####

> In Raft, the leader handles inconsistencies by forcing the followers'logs to
> duplicate its own.

Leader强制让Followers的Logs和自己保持一致，对Follower中不一致的日志项，删除。大致做法分两步：首先，从后往前找到第1条一致的日志项，根据Log Matching Property，这条日志项前面的所有日志项都跟Leader保持一致了。对这条日志项后面的日志项，先在Follower中删除，然后从Leader复制。

从具体实现来说，Leader为每个Follower维护1个*nextIndex*索引，Leader当选后先将*nextIndex*初始化为本地最后1条日志项加1。如果Follower的日志和Leader日志不一致，那么它会拒绝AppendEntries RPC，Leader被拒绝后递减对应的*nextIndex*索引，再继续尝试。最后，到达两者一致的那条日志项时，AppendEntry RPC会成功，此时在Follower中删除不一致的日志项，使Leader和Follower的日志一致。


**Tips**

对不一致的日志，前面所述方法，1个AppendEntries RPC只能检查出1条不一致的日志项，效率低。Follower发现不一致时将不一致日志项所在的term一并返回，Leader直接跳过这个term中的所有日志项。这种优化，在复制Leader时可能会重复发送一致的日志项。这时要求RPC满足幂等性，Follower要丢弃已经存在的日志项。


ceph paxos的处理方式

### Safety ###

**目标**：保证每个节点状态机执行命令的顺序相同。

#### Leader Completeness Property ####

> If a log entry is committed in a given term, then that entry will be present
> in the logs of the leaders for all higher-numbered terms.

新Leader的Log中包含所有已commit的日志项，这就是 **Leader Completeness Property**。


两种方法可以达到这个目的：一种是新Leader选举出来后开始服务前先向所有Follower同步数据，第二种是Leader选举过程中直接选择一个日志中包含所有已commit日志项的节点。Raft选择第二种方法，Ceph Paxos选择第一种方法。

1条日志项被commit意味着这条日志项已经被复制到了多数派。
1个节点当选为Leader也必须有多数派的支持。也就是说，这个多数派中至少有1个节点包含了最近commit的日志项，而节点投票给Leader的条件是Leader最近commit的日志项要不比本节点的最近commit的日志项旧。所以Leader包含了最近commit的日志项，从而包含所有已commit的日志项。

具体实现来说，RequestVote RPC包含lastLogIndex和lastLongTerm，如果Follower接收到后与自己进行比较，如果对方比较旧，则拒绝投票给它。


参考论文Figure8：

| 阶段 | term | Leader               | 日志项                      |
| :--  | :--  | :--                  | :--                         |
| a    | 2    | S1                   | 2被复制到S1、S2             |
| b    | 3    | S5                   | 3写到S5本地                 |
| c    | 4    | S1                   | 2被复制到S1、S2、S3(new)    |
| d    | 5    | S5（S2、S3、S4投票） | 3被复制到S1、S2、S3、S4、S5 |

问题：在阶段c中，日志项2已经commit（日志项2被复制到多数派），但在阶段d中，日志项2被覆盖删除。
产生问题的原因是，Leader复制旧term的日志项到其它节点。

解决这个问题的方法是，Follower只提交Leader当前Term的日志项。根据 Log Matching Property 当前Term的日志项被提交，它前面的日志项也间接地被提交了。（ *具体实现来说，Follower接收前面term的日志项保持在内存中，但先不写磁盘* ）


**Leader Completeness Property**的证明。

反证法：

假设Leader T在term T内commit了一条日志项，这条日志项在将来的term的新Leader中不存在了，假设不存在这条日志项的最小term为U，U>T。

1. 在Leader U选举阶段，那条已经提交的日志项就不存在了（否则，不会出现Leader U的日志中不存在那条日志的情况）；
2. Leader T将日志项复制到了多数派，Leader U能赢得选举也必须有多数派的支持。那么至少存在这么一个节点，既复制了来自Leader T的日志项，也将选票投给了Leader U。
3. Voter（投票者）必是先复制了来自Leader T的日志项，然后再投票给Leader U。否则，它会拒绝Leader T的AppendEntries RPC，因为Voter的currentTerm会大于T（投票给Leader U的条件是currentTerm不小于U，而U>T，所以currentTerm > T）。
4. Voter在投票给Leader U时仍然持有这条已提交的日志项。因为根据假设中间的Leader都包含这条日志项，而Leader不会删除日志项，Follower只会删除和Leader有冲突的日志项。
5. Voter投票给Leader U，所以Leader U的日志不会比Voter旧。这就开始引出冲突点了。
6. 首先，如果Voter和Leader U的日志中最后一个term相同，那么Leader U的日志至少跟Voter是等长的，也就是说，Leader U包含Voter的所有日志项。 这就有了第一个冲突点，根据假设Leader U不包含那条已提交的日志项，而Voter包含已提交的日志项。

7. 除此之外，如果Leader U的日志的最后一个term大于Voter的最后一个term。那么Leader U的最后一个term也是大于T的，因为Voter的最后一个term不小于T（因为Voter已经复制了Leader T的日志项）。根据假设，Leader U的前一个Leader包含已提交的日志项，根据Log Matching Property，Leader U也必须包含这条日志项。

8. 假设失败，所以，所有大于T的term的Leader都包含所有在term T中提交的日志项，成立。
9. Log Matching Property保证新Leader会包含所有已提交的日志项。


#### State Machine Safety Property ####

> If a server has applied a log entry at a given index to its state machine, no
> other server will ever apply a different log entry for the same index.

所有server在相同index中apply的日志项都相同。


### Follower、Candidate 故障处理 ###

相比于Leader故障的情况，Follower和Candidate的故障处理更简单，并且两者的处理方式相同：

1. RPC重试，直到节点重新启动。
2. RPC幂等性，对相同请求的多次处理结果相同。例如处理AppendEntries时对已经存在的日志项直接忽略。

### Timing and Available ###

Raft的Safety不依赖时钟，但可用性依赖时钟，可用性是指系统对client请求的响应。

```
broadcastTime << electionTimeout << MTBF
```

broadcastTime 广播时间， 1台server并发发送RPC给集群中所有server，并且收到回复的平均时间；
electionTimeout 选举的超时时间；
MTBF 平均故障间隔，单台server相邻两次故障的平均时间间隔。

broadcastTime依赖于磁盘性能，因为RPC会要求持久化一些数据到磁盘，范围在0.5ms到20ms之间。
electionTimeout 可以设置为10ms到500ms之间。
MTBF 一般是几个月或者更长时间，所以一定是能够满足要求的。


要求broadcastTime比electionTimeout低一个数量级：
1. Leader可以发送稳定的心跳以阻止Follower启动新一轮的选举；
2. 每个server的electionTimeout是随机的，这个要求可以避免出现split vote问题。

要求electionTimeout比MTBF低一个数量级：
1. Leader故障后，系统将有一段时间是不可用的，这段时间大致是electionTimeout。应该尽力避免这种情况。


### Cluster membership Change ###

有两种配置更新方式，一种是离线更新，一种是在线更新。离线更新会导致服务不可用，不可取。在线更新的问题是可能出现双Leader问题。如图Figure10，将集群从3个节点增加到5个节点。Server1和Server2使用旧配置选出一个Leader，Server3、Server4和Server5使用新配置选出另一个Leader。

解决这个问题要采用two-phase的方法。一种解法是，先在第一阶段disable旧配置，期间不能处理client请求；再在第二阶段使能新配置。
Raft的做法是先切换到过渡期的配置，称为`joint consensus`；一旦`joint consensus`提交后，系统切换到新配置。`joint consensus` 由旧配置和新配置组合而成：

1. 复制日志项到所有新、旧配置的server；
2. 新、旧配置的server都可以成为Leader；
3. 达成一致（针对Leader选举、日志项提交）需要分别在新旧两种配置上获得多数派的支持。

Figure 11 图解。
虚线代表日志项已创建但还没提交，实线代表最新提交的日志项。
Leader先创建Cold,new日志项并提交，提交过程要求Cold的多数派和Cnew的多数派支持。
然后创建Cnew日志项并提交到Cnew对应的多数派。


server一把新配置添加到自己的日志，后续就采用新配置来做决策，而不管这条日志是否已经提交。
Leader要使用C-old,new来确定C-old,new 是否已提交。

































